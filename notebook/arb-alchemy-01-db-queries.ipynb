{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from db import *\n",
    "from sqlalchemy import func\n",
    "from sqlalchemy.orm import aliased\n",
    "import numpy as np\n",
    "from sqlalchemy import or_, and_\n",
    "from tqdm import tqdm\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "db_loc = '../scratch/test.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(f\"sqlite:///{db_loc}\")\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = (\n",
    "    session.query(PaperKeywords.paper_bibcode, PaperKeywords.raw_keyword, PaperKeywords.score, Paper)\n",
    "    .join(Paper)\n",
    "    .filter(PaperKeywords.raw_keyword == 'black hole')\n",
    "    .filter(PaperKeywords.score == None)\n",
    "    .order_by(func.length(Paper.abstract))\n",
    "    .first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microscopic origin of the Bekenstein-Hawking entropy. The Bekenstein-Hawking area-entropy relation SBH = A/(4) is derived for a class of five-dimensional extremal black holes in string theory by counting the degeneracy of BPS soliton bound states. '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh[-1].get_feature_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh2 = (\n",
    "    session.query(PaperKeywords.paper_bibcode, PaperKeywords.raw_keyword, PaperKeywords.score, Paper)\n",
    "    .join(Paper)\n",
    "    .filter(PaperKeywords.raw_keyword == 'black hole')\n",
    "    .filter(PaperKeywords.score != None)\n",
    "    .order_by(func.length(Paper.abstract))\n",
    "    .first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Black Holes in Higher Dimensions. We review black-hole solutions of higher-dimensional vacuum gravity and higher-dimensional supergravity theories. The discussion of vacuum gravity is pedagogical, with detailed reviews of Myers-Perry solutions, black rings, and solution-generating techniques. We discuss black-hole solutions of maximal supergravity theories, including black holes in anti-de Sitter space. General results and open problems are discussed throughout. '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh2[-].get_feature_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('state', 0.011182539771310823),\n",
       " ('lack', None),\n",
       " ('relation', None),\n",
       " ('class', 0.03279630223045727),\n",
       " ('black hole', None),\n",
       " ('origin', None),\n",
       " ('string theory', 0.06890874950642653),\n",
       " ('area', None),\n",
       " ('theory', None),\n",
       " ('degeneracy', 0.02655961718559421),\n",
       " ('five-dimensional extremal black hole', 0.13282932929223656),\n",
       " ('area-entropy relation SBH', 0.10100352236833343),\n",
       " ('BPS soliton', 0.03817872221387163),\n",
       " ('A/(4', 0.03301590089145787),\n",
       " ('microscopic origin', 0.01872708362609707)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(p.raw_keyword, p.score) for p in bh[-1].keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = (\n",
    "    session.query(PaperKeywords.paper_bibcode, PaperKeywords.raw_keyword, PaperKeywords.score, Paper)\n",
    "    .join(Paper)\n",
    "    .filter(PaperKeywords.raw_keyword == 'black hole')\n",
    "    .filter(PaperKeywords.score == None)\n",
    "    .first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2010MNRAS.406.2650M',\n",
       " 'black hole',\n",
       " None,\n",
       " <Paper(bibcode=\"2010MNRAS.406.2650M\", title=\"Electromagnetic counterparts of compact object mergers powered by the radioactive decay of r-process nuclei\")>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = PaperOrganizer(no_below=10, no_above=0.5, min_mean_score=0.01, year_min=2000, year_max=2007, journal_blacklist=[\"arXiv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14429)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po._get_filtered_keywords(session, Keyword.id).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwd_query = po._get_filtered_keywords(session)\n",
    "\n",
    "# print(po.journal_blacklist)\n",
    "\n",
    "# for j in po.journal_blacklist:\n",
    "#     kwd_query = kwd_query.filter(~Paper.bibcode.contains(j))\n",
    "\n",
    "# kwd_query[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = session.query(Paper).count()\n",
    "no_above_abs = int(po.no_above * corpus_size)\n",
    "kwd_query = (\n",
    "    session.query(\n",
    "        Keyword, func.count(Keyword.id), func.avg(PaperKeywords.score)\n",
    "    )\n",
    "    .join(PaperKeywords)\n",
    "    .join(Paper)\n",
    "    .group_by(Keyword.id)\n",
    "    .order_by(func.avg(PaperKeywords.score).desc())\n",
    "    .having(func.count() >= po.no_below)\n",
    "    .having(func.count() <= no_above_abs)\n",
    "    .having(func.avg(PaperKeywords.score) >= po.min_mean_score)\n",
    ")\n",
    "for j in po.journal_blacklist:\n",
    "    kwd_query = kwd_query.filter(~Paper.bibcode.contains(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds, _, _ = zip(*kwd_query.all())\n",
    "kwd_ids = [k.id for k in kwds]\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "q = (\n",
    "    session.query(PaperKeywords.paper_bibcode, PaperKeywords.keyword_id, PaperKeywords.count)\n",
    "    .filter(PaperKeywords.keyword_id.in_(kwd_ids))\n",
    "    .group_by(PaperKeywords.paper_bibcode)\n",
    "    .order_by(PaperKeywords.paper_bibcode)\n",
    ")\n",
    "\n",
    "bibs, keyword_ids, counts = zip(*q)\n",
    "\n",
    "corp_ind_to_bib = {i: b for i, b in enumerate(set(bibs))}\n",
    "bib_to_corp_ind = {b: i for i, b in id_to_bib.items()}\n",
    "\n",
    "dct_ind_to_kwd_ind = {i: k for i, k in enumerate(set(keyword_ids))}\n",
    "kwd_id_to_dct_ind = {k: i for i, k in dct_ind_to_kwd_ind.items()}\n",
    "\n",
    "corp_inds = [bib_to_corp_ind[b] for b in bibs]\n",
    "dct_inds = [kwd_id_to_dct_ind[k] for k in keyword_ids]\n",
    "\n",
    "coo_corpus =((b, k, c) for b, k, c in zip(corp_inds, dct_inds, counts))\n",
    "a = np.fromiter(coo_corpus, dtype=[('row', int), ('col', int), ('value', int)])\n",
    "m = coo_matrix((a['value'], (a['row'], a['col'])))\n",
    "corpus = Sparse2Corpus(m)\n",
    "\n",
    "q = session.query(Keyword.id, Keyword.keyword).filter(Keyword.id.in_(set(keyword_ids)))\n",
    "id2word = {kwd_id_to_dct_ind[i]: k for i, k in q}\n",
    "dct = Dictionary.from_corpus(corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = session.query(Paper).count()\n",
    "no_above_abs = int(po.no_above * corpus_size)\n",
    "kwd_query = (\n",
    "    session.query(\n",
    "        Keyword, func.count(Keyword.id), func.avg(PaperKeywords.score)\n",
    "    )\n",
    "    .join(PaperKeywords)\n",
    "    .join(Paper)\n",
    "    .group_by(Keyword.id)\n",
    "    .order_by(func.avg(PaperKeywords.score).desc())\n",
    "    .having(func.count() >= po.no_below)\n",
    "    .having(func.count() <= no_above_abs)\n",
    "    .having(func.avg(PaperKeywords.score) >= po.min_mean_score)\n",
    ")\n",
    "for j in po.journal_blacklist:\n",
    "    kwd_query = kwd_query.filter(~Paper.bibcode.contains(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwd_query.group_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_range = [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:52800\n",
      "distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:52802'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:52803'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:52804'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:52805'\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:52806', name: 0, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:52806\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:52808', name: 3, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:52808\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:52810', name: 1, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:52810\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:52812', name: 2, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:52812\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Receive client connection: Client-e3a739ba-b354-11ea-a411-acde48001122\n",
      "distributed.core - INFO - Starting established connection\n",
      "INFO:db:Dask dashboard: http://127.0.0.1:8787/status\n",
      "100%|██████████| 7245/7245 [00:12<00:00, 570.20it/s]\n",
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(1572 unique tokens: ['angular resolution', 'useful tool', 'gas giant planet', 'habitable zone', 'orbital period']...) from 7245 documents (total 45135 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "models = po.make_all_topic_models(session, topic_range, alpha=\"auto\", eta=\"auto\", iterations=200, passes=5, eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4fc2685c9fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:50872', name: tcp://127.0.0.1:50872, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:50872\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:50872', name: tcp://127.0.0.1:50872, memory: 0, processing: 0>\n",
      "distributed.core - INFO - Removing comms to tcp://127.0.0.1:50872\n"
     ]
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_topics=10:   0%|          | 0/3 [00:00<?, ?it/s]DEBUG:gensim.models.coherencemodel:Setting topics to those of the model: LdaModel(num_terms=1572, num_topics=10, decay=0.5, chunksize=2000)\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "n_topics=20:   0%|          | 0/3 [00:00<?, ?it/s]DEBUG:gensim.models.coherencemodel:Setting topics to those of the model: LdaModel(num_terms=1572, num_topics=20, decay=0.5, chunksize=2000)\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "n_topics=30:  67%|██████▋   | 2/3 [00:00<00:00, 16.46it/s]DEBUG:gensim.models.coherencemodel:Setting topics to those of the model: LdaModel(num_terms=1572, num_topics=30, decay=0.5, chunksize=2000)\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 7000 documents\n",
      "n_topics=30: 100%|██████████| 3/3 [00:00<00:00, 14.57it/s]\n",
      "distributed.utils - ERROR - 'deserialize-make_topic_model'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/utils.py\", line 665, in log_errors\n",
      "    yield\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 312, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'deserialize-make_topic_model'\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x131c13c90>>, <Future finished exception=KeyError('deserialize-make_topic_model')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/ioloop.py\", line 767, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/server/session.py\", line 70, in _needs_document_lock_wrapper\n",
      "    result = yield yield_for_all_futures(func(self, *args, **kwargs))\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/server/session.py\", line 191, in with_document_locked\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1127, in wrapper\n",
      "    return doc._with_self_as_curdoc(invoke)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1113, in _with_self_as_curdoc\n",
      "    return f()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1126, in invoke\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 916, in remove_then_invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 333, in <lambda>\n",
      "    self.doc().add_next_tick_callback(lambda: self.update(prof, metadata))\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/core/property/validation.py\", line 97, in func\n",
      "    return input_function(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 312, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'deserialize-make_topic_model'\n",
      "distributed.utils - ERROR - 'deserialize-make_topic_model'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/utils.py\", line 665, in log_errors\n",
      "    yield\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 312, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'deserialize-make_topic_model'\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x131c13c90>>, <Future finished exception=KeyError('deserialize-make_topic_model')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/ioloop.py\", line 767, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/server/session.py\", line 70, in _needs_document_lock_wrapper\n",
      "    result = yield yield_for_all_futures(func(self, *args, **kwargs))\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/server/session.py\", line 191, in with_document_locked\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1127, in wrapper\n",
      "    return doc._with_self_as_curdoc(invoke)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1113, in _with_self_as_curdoc\n",
      "    return f()\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 1126, in invoke\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/document/document.py\", line 916, in remove_then_invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 333, in <lambda>\n",
      "    self.doc().add_next_tick_callback(lambda: self.update(prof, metadata))\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/bokeh/core/property/validation.py\", line 97, in func\n",
      "    return input_function(*args, **kwargs)\n",
      "  File \"/Users/abuonomo/code/DataSquad/astro2020/venv/lib/python3.7/site-packages/distributed/dashboard/components/shared.py\", line 312, in update\n",
      "    ts = metadata[\"keys\"][self.key]\n",
      "KeyError: 'deserialize-make_topic_model'\n"
     ]
    }
   ],
   "source": [
    "cohs = po.get_coherences(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-12.462846980899453, -15.21323348973292, -16.516400239083705]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7245/7245 [00:14<00:00, 511.87it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = po.get_tokens(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwd_ids = [k.id for k, _, _ in kwds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = session.query(Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = papers.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens0 = [[pk.keyword.keyword] * pk.count for pk in p.keywords] #if pk.keyword_id in kwd_ids]\n",
    "tokens = [t for ts in tokens0 for t in ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky survey',\n",
       " 'sky survey',\n",
       " 'Wide-field Infrared Survey Explorer',\n",
       " 'Wide-field Infrared Survey Explorer',\n",
       " 'European Southern Observatory Schmidt',\n",
       " 'point source sensitivity well',\n",
       " 'Palomar Observatory Schmidt',\n",
       " 'United Kingdom Schmidt',\n",
       " 'high signal-to-noise source',\n",
       " 'WISE',\n",
       " 'WISE',\n",
       " 'WISE',\n",
       " 'WISE',\n",
       " 'Initial On-orbit Performance',\n",
       " 'InfraRed Astronomical Satellite',\n",
       " 'denser coverage',\n",
       " 'ecliptic pole',\n",
       " 'useful tool',\n",
       " 'second time',\n",
       " 'Mission Description',\n",
       " 'astrometric precision',\n",
       " 'zodiacal background',\n",
       " 'angular resolution',\n",
       " 'unconfused region',\n",
       " 'μm',\n",
       " 'μm',\n",
       " 'astronomy',\n",
       " 'value',\n",
       " 'decade',\n",
       " 'Micron',\n",
       " 'cryogen',\n",
       " 'November',\n",
       " 'launch',\n",
       " 'December',\n",
       " 'mJy',\n",
       " 'July',\n",
       " 'band',\n",
       " 'January',\n",
       " 'wavelength',\n",
       " '0farcs15']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# session.query(PaperKeywords).join(Paper).filter(PaperKeywords.raw_keyword == \"gamma\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "year_counts = session.query(Paper.year, func.count(Paper.year)).group_by(Paper.year).all()\n",
    "\n",
    "session.query(Keyword).filter(Keyword.keyword == \"black hole\").first().get_years(session)\n",
    "\n",
    "session.query(Paper.year, func.count(Paper.year)) \\\n",
    "    .join(PaperKeywords) \\\n",
    "    .join(Keyword) \\\n",
    "    .filter(Keyword.keyword.ilike('mars')) \\\n",
    "    .group_by(Paper.year) \\\n",
    "    .all()\n",
    "\n",
    "session.query(Keyword).filter(Keyword.keyword.ilike('universe'))[0].papers[1].paper.get_feature_text()\n",
    "\n",
    "!pip install snakeviz\n",
    "\n",
    "%load_ext snakeviz\n",
    "\n",
    "paper_kwd = session.query(PaperKeywords).join(Keyword).filter(Keyword.keyword==\"measurement\").first()\n",
    "\n",
    "%%timeit -n 1 -r 1\n",
    "added = 0\n",
    "q = (\n",
    "    session.query(Paper)\n",
    "    .filter(~Paper.keywords.any(PaperKeywords.keyword_id == paper_kwd.keyword_id))\n",
    "    .filter(\n",
    "        or_(\n",
    "            Paper.title.contains(paper_kwd.raw_keyword), \n",
    "            Paper.abstract.contains(paper_kwd.raw_keyword)\n",
    "        )\n",
    "    ).update\n",
    ")\n",
    "# %%timeit\n",
    "# papers = q.all()\n",
    "# with session.no_autoflush:\n",
    "#     for p in papers:\n",
    "#     #     with session.no_autoflush:\n",
    "#         assoc = PaperKeywords(raw_keyword=paper_kwd.raw_keyword, score=np.nan)\n",
    "#         assoc.keyword = paper_kwd.keyword\n",
    "#         p.keywords.append(assoc)\n",
    "#         added += 1\n",
    "# session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133320"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Keyword).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk_dict(p, paper_kwd):\n",
    "    d = {\n",
    "    'raw_keyword': paper_kwd.raw_keyword,\n",
    "    'keyword_id': paper_kwd.keyword.id,\n",
    "    'paper_bibcode': p.bibcode,\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaperKeywords(paper_bibcode=\"1997ApJ...486..665P\", keyword.keyword=\"+\")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(PaperKeywords).group_by(PaperKeywords.raw_keyword).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87466bd8d8e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_autoflush\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPaperKeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpaper_kwd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session' is not defined"
     ]
    }
   ],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "added = 0\n",
    "t = 100\n",
    "all_records = []\n",
    "with session.no_autoflush:\n",
    "    pbar = tqdm(session.query(PaperKeywords)[100:100+t], total=t)\n",
    "    for paper_kwd in pbar:\n",
    "        pbar.set_description(paper_kwd.raw_keyword)\n",
    "        q = (\n",
    "            session.query(Paper)\n",
    "            .filter(~Paper.keywords.any(PaperKeywords.keyword_id == paper_kwd.keyword_id))\n",
    "            .filter(\n",
    "                or_(\n",
    "                    Paper.title.contains(paper_kwd.raw_keyword), \n",
    "                    Paper.abstract.contains(paper_kwd.raw_keyword)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        papers = q.all()\n",
    "        for p in papers:\n",
    "            d = get_pk_dict(p, paper_kwd)\n",
    "            all_records.append(d)\n",
    "            added += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11830"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "engine.execute(\n",
    "    PaperKeywords.__table__.insert(),\n",
    "    all_records,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(PaperKeywords).join(Keyword).filter(Keyword.keyword == 'measurement').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT paper_keywords.paper_bibcode AS paper_keywords_paper_bibcode, paper_keywords.keyword_id AS paper_keywords_keyword_id, paper_keywords.score AS paper_keywords_score, paper_keywords.raw_keyword AS paper_keywords_raw_keyword \n",
      "FROM paper_keywords JOIN keywords ON keywords.id = paper_keywords.keyword_id \n",
      "WHERE keywords.keyword = ?\n"
     ]
    }
   ],
   "source": [
    "print(session.query(PaperKeywords).join(Keyword).filter(Keyword.keyword == 'measurement'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:///../scratch/test.sqlite)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(PaperKeywords).join(Keyword).filter(Keyword.keyword == 'measurement').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15513"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.rollback()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2010MNRAS.409..531R')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeywordAlias = aliased(Keyword)\n",
    "q = (\n",
    " session.query(Keyword, func.count(Keyword.id))\n",
    "    .join(PaperKeywords)\n",
    "    .join(Paper)\n",
    "    .group_by(Keyword.id)\n",
    "#     .filter(Paper.year == 2010)\n",
    "    .order_by(func.count(Keyword.id).desc())\n",
    "    .having(func.count() > 10)\n",
    "    .having(funct.count() < )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT keywords.id AS keywords_id, keywords.keyword AS keywords_keyword, count(keywords.id) AS count_1 \n",
      "FROM keywords JOIN paper_keywords ON keywords.id = paper_keywords.keyword_id JOIN papers ON papers.bibcode = paper_keywords.paper_bibcode GROUP BY keywords.id \n",
      "HAVING count(*) > ? ORDER BY count(keywords.id) DESC\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3225"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.query(Paper).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ([pk.keyword.keyword for pk in paper.keywords] for paper in session.query(Paper).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_spacy_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PaperMiner(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.get_dictionary(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pm.get_corpus(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro202",
   "language": "python",
   "name": "astro2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
